{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.4.1-cp312-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.19.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.0 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting scikit-image\n",
      "  Using cached scikit_image-0.24.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.9.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-75.2.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: numpy in /Users/shaanjijoe/MACHINE LEARNING ENVIRONMENT/env/lib/python3.12/site-packages (from torchvision) (2.1.2)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached pillow-11.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Collecting scipy>=1.9 (from scikit-image)\n",
      "  Using cached scipy-1.14.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting imageio>=2.33 (from scikit-image)\n",
      "  Using cached imageio-2.36.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
      "  Using cached tifffile-2024.9.20-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: packaging>=21 in /Users/shaanjijoe/MACHINE LEARNING ENVIRONMENT/env/lib/python3.12/site-packages (from scikit-image) (24.1)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.54.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (163 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.7-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/shaanjijoe/MACHINE LEARNING ENVIRONMENT/env/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/shaanjijoe/MACHINE LEARNING ENVIRONMENT/env/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached torch-2.4.1-cp312-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "Using cached torchvision-0.19.1-cp312-cp312-macosx_11_0_arm64.whl (1.7 MB)\n",
      "Downloading torchaudio-2.4.1-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scikit_image-0.24.0-cp312-cp312-macosx_12_0_arm64.whl (13.4 MB)\n",
      "Using cached matplotlib-3.9.2-cp312-cp312-macosx_11_0_arm64.whl (7.8 MB)\n",
      "Using cached contourpy-1.3.0-cp312-cp312-macosx_11_0_arm64.whl (251 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.54.1-cp312-cp312-macosx_11_0_arm64.whl (2.3 MB)\n",
      "Using cached imageio-2.36.0-py3-none-any.whl (315 kB)\n",
      "Using cached kiwisolver-1.4.7-cp312-cp312-macosx_11_0_arm64.whl (63 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached networkx-3.4.1-py3-none-any.whl (1.7 MB)\n",
      "Using cached pillow-11.0.0-cp312-cp312-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Using cached pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Using cached scipy-1.14.1-cp312-cp312-macosx_14_0_arm64.whl (23.1 MB)\n",
      "Using cached tifffile-2024.9.20-py3-none-any.whl (228 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached setuptools-75.2.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached MarkupSafe-3.0.1-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, typing-extensions, tifffile, sympy, setuptools, scipy, pyparsing, pillow, networkx, MarkupSafe, lazy-loader, kiwisolver, fsspec, fonttools, filelock, cycler, contourpy, matplotlib, jinja2, imageio, torch, scikit-image, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-3.0.1 contourpy-1.3.0 cycler-0.12.1 filelock-3.16.1 fonttools-4.54.1 fsspec-2024.9.0 imageio-2.36.0 jinja2-3.1.4 kiwisolver-1.4.7 lazy-loader-0.4 matplotlib-3.9.2 mpmath-1.3.0 networkx-3.4.1 pillow-11.0.0 pyparsing-3.2.0 scikit-image-0.24.0 scipy-1.14.1 setuptools-75.2.0 sympy-1.13.3 tifffile-2024.9.20 torch-2.4.1 torchaudio-2.4.1 torchvision-0.19.1 typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio scikit-image matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary tools (libraries) to work with data, images, and neural networks\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "import torch.utils.data as data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: /Users/shaanjijoe/MACHINE_LEARNING_ENVIRONMENT/Tuberculosis_dataset/ChinaSet_AllFiles/ChinaSet_AllFiles/CXR_png\n",
      "['CHNCXR_0492_1.png', 'CHNCXR_0187_0.png', 'CHNCXR_0342_1.png', 'CHNCXR_0286_0.png', 'CHNCXR_0022_0.png', 'CHNCXR_0067_0.png', 'CHNCXR_0572_1.png', 'CHNCXR_0537_1.png', 'CHNCXR_0323_0.png', 'CHNCXR_0636_1.png', 'CHNCXR_0147_0.png', 'CHNCXR_0102_0.png', 'CHNCXR_0417_1.png', 'CHNCXR_0452_1.png', 'CHNCXR_0246_0.png', 'Thumbs.db', 'CHNCXR_0203_0.png', 'CHNCXR_0382_1.png', 'CHNCXR_0083_0.png', 'CHNCXR_0596_1.png', 'CHNCXR_0262_0.png', 'CHNCXR_0227_0.png', 'CHNCXR_0433_1.png', 'CHNCXR_0476_1.png', 'CHNCXR_0163_0.png', 'CHNCXR_0126_0.png', 'CHNCXR_0366_1.png', 'CHNCXR_0657_1.png', 'CHNCXR_0612_1.png', 'CHNCXR_0307_0.png', 'CHNCXR_0556_1.png', 'CHNCXR_0513_1.png', 'CHNCXR_0006_0.png', 'CHNCXR_0043_0.png', 'CHNCXR_0218_0.png', 'CHNCXR_0399_1.png', 'CHNCXR_0119_0.png', 'CHNCXR_0364_1.png', 'CHNCXR_0449_1.png', 'CHNCXR_0004_0.png', 'CHNCXR_0041_0.png', 'CHNCXR_0554_1.png', 'CHNCXR_0511_1.png', 'CHNCXR_0305_0.png', 'CHNCXR_0655_1.png', 'CHNCXR_0610_1.png', 'CHNCXR_0594_1.png', 'CHNCXR_0081_0.png', 'CHNCXR_0039_0.png', 'CHNCXR_0569_1.png', 'CHNCXR_0359_1.png', 'CHNCXR_0161_0.png', 'CHNCXR_0124_0.png', 'CHNCXR_0431_1.png', 'CHNCXR_0474_1.png', 'CHNCXR_0489_1.png', 'CHNCXR_0260_0.png', 'CHNCXR_0225_0.png', 'CHNCXR_0508_1.png', 'CHNCXR_0058_0.png', 'CHNCXR_0609_1.png', 'CHNCXR_0380_1.png', 'CHNCXR_0244_0.png', 'CHNCXR_0201_0.png', 'CHNCXR_0415_1.png', 'CHNCXR_0450_1.png', 'CHNCXR_0145_0.png', 'CHNCXR_0338_1.png', 'CHNCXR_0100_0.png', 'CHNCXR_0284_0.png', 'CHNCXR_0428_1.png', 'CHNCXR_0178_0.png', 'CHNCXR_0340_1.png', 'CHNCXR_0185_0.png', 'CHNCXR_0279_0.png', 'CHNCXR_0490_1.png', 'CHNCXR_0098_0.png', 'CHNCXR_0634_1.png', 'CHNCXR_0321_0.png', 'CHNCXR_0570_1.png', 'CHNCXR_0535_1.png', 'CHNCXR_0020_0.png', 'CHNCXR_0065_0.png', 'CHNCXR_0411_1.png', 'CHNCXR_0454_1.png', 'CHNCXR_0379_1.png', 'CHNCXR_0141_0.png', 'CHNCXR_0104_0.png', 'CHNCXR_0384_1.png', 'CHNCXR_0240_0.png', 'CHNCXR_0205_0.png', 'CHNCXR_0648_1.png', 'CHNCXR_0318_0.png', 'CHNCXR_0549_1.png', 'CHNCXR_0019_0.png', 'CHNCXR_0574_1.png', 'CHNCXR_0531_1.png', 'CHNCXR_0024_0.png', 'CHNCXR_0061_0.png', 'CHNCXR_0630_1.png', 'CHNCXR_0325_0.png', 'CHNCXR_0589_1.png', 'CHNCXR_0181_0.png', 'CHNCXR_0238_0.png', 'CHNCXR_0494_1.png', 'CHNCXR_0280_0.png', 'CHNCXR_0469_1.png', 'CHNCXR_0139_0.png', 'CHNCXR_0344_1.png', 'CHNCXR_0301_0.png', 'CHNCXR_0651_1.png', 'CHNCXR_0614_1.png', 'CHNCXR_0045_0.png', 'CHNCXR_0550_1.png', 'CHNCXR_0515_1.png', 'CHNCXR_0158_0.png', 'CHNCXR_0360_1.png', 'CHNCXR_0408_1.png', 'CHNCXR_0259_0.png', 'CHNCXR_0264_0.png', 'CHNCXR_0221_0.png', 'CHNCXR_0198_0.png', 'CHNCXR_0165_0.png', 'CHNCXR_0120_0.png', 'CHNCXR_0435_1.png', 'CHNCXR_0470_1.png', 'CHNCXR_0299_0.png', 'CHNCXR_0078_0.png', 'CHNCXR_0528_1.png', 'CHNCXR_0590_1.png', 'CHNCXR_0629_1.png', 'CHNCXR_0085_0.png', 'CHNCXR_0437_1.png', 'CHNCXR_0472_1.png', 'CHNCXR_0167_0.png', 'CHNCXR_0122_0.png', 'CHNCXR_0266_0.png', 'CHNCXR_0223_0.png', 'CHNCXR_0087_0.png', 'CHNCXR_0592_1.png', 'CHNCXR_0552_1.png', 'CHNCXR_0517_1.png', 'CHNCXR_0002_0.png', 'CHNCXR_0047_0.png', 'CHNCXR_0653_1.png', 'CHNCXR_0616_1.png', 'CHNCXR_0303_0.png', 'CHNCXR_0327_1.png', 'CHNCXR_0362_1.png', 'CHNCXR_0632_1.png', 'CHNCXR_0026_0.png', 'CHNCXR_0063_0.png', 'CHNCXR_0576_1.png', 'CHNCXR_0533_1.png', 'CHNCXR_0346_1.png', 'CHNCXR_0282_0.png', 'CHNCXR_0496_1.png', 'CHNCXR_0183_0.png', 'CHNCXR_0242_0.png', 'CHNCXR_0207_0.png', 'CHNCXR_0386_1.png', 'CHNCXR_0143_0.png', 'CHNCXR_0106_0.png', 'CHNCXR_0413_1.png', 'CHNCXR_0456_1.png', 'CHNCXR_0125_0.png', 'CHNCXR_0358_1.png', 'CHNCXR_0160_0.png', 'CHNCXR_0475_1.png', 'CHNCXR_0430_1.png', 'CHNCXR_0224_0.png', 'CHNCXR_0261_0.png', 'CHNCXR_0488_1.png', 'CHNCXR_0595_1.png', 'CHNCXR_0080_0.png', 'CHNCXR_0038_0.png', 'CHNCXR_0568_1.png', 'CHNCXR_0040_0.png', 'CHNCXR_0005_0.png', 'CHNCXR_0510_1.png', 'CHNCXR_0555_1.png', 'CHNCXR_0304_0.png', 'CHNCXR_0611_1.png', 'CHNCXR_0654_1.png', 'CHNCXR_0219_0.png', 'CHNCXR_0398_1.png', 'CHNCXR_0365_1.png', 'CHNCXR_0118_0.png', 'CHNCXR_0448_1.png', 'CHNCXR_0635_1.png', 'CHNCXR_0099_0.png', 'CHNCXR_0320_0.png', 'CHNCXR_0534_1.png', 'CHNCXR_0571_1.png', 'CHNCXR_0064_0.png', 'CHNCXR_0021_0.png', 'CHNCXR_0429_1.png', 'CHNCXR_0285_0.png', 'CHNCXR_0179_0.png', 'CHNCXR_0341_1.png', 'CHNCXR_0184_0.png', 'CHNCXR_0491_1.png', 'CHNCXR_0278_0.png', 'CHNCXR_0381_1.png', 'CHNCXR_0200_0.png', 'CHNCXR_0245_0.png', 'CHNCXR_0451_1.png', 'CHNCXR_0414_1.png', 'CHNCXR_0339_1.png', 'CHNCXR_0101_0.png', 'CHNCXR_0144_0.png', 'CHNCXR_0509_1.png', 'CHNCXR_0059_0.png', 'CHNCXR_0608_1.png', 'CHNCXR_0103_0.png', 'CHNCXR_0146_0.png', 'CHNCXR_0453_1.png', 'CHNCXR_0416_1.png', 'CHNCXR_0202_0.png', 'CHNCXR_0247_0.png', 'CHNCXR_0383_1.png', 'CHNCXR_0066_0.png', 'CHNCXR_0023_0.png', 'CHNCXR_0536_1.png', 'CHNCXR_0573_1.png', 'CHNCXR_0322_0.png', 'CHNCXR_0637_1.png', 'CHNCXR_0493_1.png', 'CHNCXR_0186_0.png', 'CHNCXR_0343_1.png', 'CHNCXR_0287_0.png', 'CHNCXR_0613_1.png', 'CHNCXR_0656_1.png', 'CHNCXR_0306_0.png', 'CHNCXR_0512_1.png', 'CHNCXR_0557_1.png', 'CHNCXR_0042_0.png', 'CHNCXR_0007_0.png', 'CHNCXR_0367_1.png', 'CHNCXR_0226_0.png', 'CHNCXR_0263_0.png', 'CHNCXR_0477_1.png', 'CHNCXR_0432_1.png', 'CHNCXR_0127_0.png', 'CHNCXR_0162_0.png', 'CHNCXR_0082_0.png', 'CHNCXR_0597_1.png', 'CHNCXR_0363_1.png', 'CHNCXR_0516_1.png', 'CHNCXR_0553_1.png', 'CHNCXR_0046_0.png', 'CHNCXR_0003_0.png', 'CHNCXR_0617_1.png', 'CHNCXR_0652_1.png', 'CHNCXR_0302_0.png', 'CHNCXR_0086_0.png', 'CHNCXR_0593_1.png', 'CHNCXR_0473_1.png', 'CHNCXR_0436_1.png', 'CHNCXR_0123_0.png', 'CHNCXR_0166_0.png', 'CHNCXR_0222_0.png', 'CHNCXR_0267_0.png', 'CHNCXR_0206_0.png', 'CHNCXR_0243_0.png', 'CHNCXR_0387_1.png', 'CHNCXR_0107_0.png', 'CHNCXR_0142_0.png', 'CHNCXR_0457_1.png', 'CHNCXR_0412_1.png', 'CHNCXR_0347_1.png', 'CHNCXR_0283_0.png', 'CHNCXR_0497_1.png', 'CHNCXR_0182_0.png', 'CHNCXR_0326_0.png', 'CHNCXR_0633_1.png', 'CHNCXR_0062_0.png', 'CHNCXR_0027_0.png', 'CHNCXR_0532_1.png', 'CHNCXR_0577_1.png', 'CHNCXR_0180_0.png', 'CHNCXR_0495_1.png', 'CHNCXR_0239_0.png', 'CHNCXR_0468_1.png', 'CHNCXR_0281_0.png', 'CHNCXR_0345_1.png', 'CHNCXR_0138_0.png', 'CHNCXR_0530_1.png', 'CHNCXR_0575_1.png', 'CHNCXR_0060_0.png', 'CHNCXR_0025_0.png', 'CHNCXR_0631_1.png', 'CHNCXR_0588_1.png', 'CHNCXR_0324_0.png', 'CHNCXR_0649_1.png', 'CHNCXR_0319_0.png', 'CHNCXR_0548_1.png', 'CHNCXR_0018_0.png', 'CHNCXR_0455_1.png', 'CHNCXR_0410_1.png', 'CHNCXR_0105_0.png', 'CHNCXR_0378_1.png', 'CHNCXR_0140_0.png', 'CHNCXR_0385_1.png', 'CHNCXR_0204_0.png', 'CHNCXR_0241_0.png', 'CHNCXR_0079_0.png', 'CHNCXR_0529_1.png', 'CHNCXR_0591_1.png', 'CHNCXR_0084_0.png', 'CHNCXR_0628_1.png', 'CHNCXR_0220_0.png', 'CHNCXR_0265_0.png', 'CHNCXR_0199_0.png', 'CHNCXR_0121_0.png', 'CHNCXR_0164_0.png', 'CHNCXR_0298_0.png', 'CHNCXR_0471_1.png', 'CHNCXR_0434_1.png', 'CHNCXR_0159_0.png', 'CHNCXR_0361_1.png', 'CHNCXR_0409_1.png', 'CHNCXR_0258_0.png', 'CHNCXR_0300_0.png', 'CHNCXR_0615_1.png', 'CHNCXR_0650_1.png', 'CHNCXR_0044_0.png', 'CHNCXR_0001_0.png', 'CHNCXR_0514_1.png', 'CHNCXR_0551_1.png', 'CHNCXR_0405_1.png', 'CHNCXR_0440_1.png', 'CHNCXR_0155_0.png', 'CHNCXR_0328_1.png', 'CHNCXR_0110_0.png', 'CHNCXR_0390_1.png', 'CHNCXR_0254_0.png', 'CHNCXR_0211_0.png', 'CHNCXR_0619_1.png', 'CHNCXR_0518_1.png', 'CHNCXR_0048_0.png', 'CHNCXR_0560_1.png', 'CHNCXR_0525_1.png', 'CHNCXR_0030_0.png', 'CHNCXR_0075_0.png', 'CHNCXR_0661_1.png', 'CHNCXR_0088_0.png', 'CHNCXR_0624_1.png', 'CHNCXR_0195_0.png', 'CHNCXR_0269_0.png', 'CHNCXR_0480_1.png', 'CHNCXR_0294_0.png', 'CHNCXR_0438_1.png', 'CHNCXR_0168_0.png', 'CHNCXR_0350_1.png', 'CHNCXR_0315_0.png', 'CHNCXR_0645_1.png', 'CHNCXR_0600_1.png', 'CHNCXR_0014_0.png', 'CHNCXR_0051_0.png', 'CHNCXR_0544_1.png', 'CHNCXR_0501_1.png', 'CHNCXR_0109_0.png', 'CHNCXR_0331_1.png', 'CHNCXR_0374_1.png', 'CHNCXR_0459_1.png', 'CHNCXR_0208_0.png', 'CHNCXR_0389_1.png', 'CHNCXR_0499_1.png', 'CHNCXR_0270_0.png', 'CHNCXR_0235_0.png', 'CHNCXR_0349_1.png', 'CHNCXR_0171_0.png', 'CHNCXR_0134_0.png', 'CHNCXR_0421_1.png', 'CHNCXR_0464_1.png', 'CHNCXR_0029_0.png', 'CHNCXR_0579_1.png', 'CHNCXR_0584_1.png', 'CHNCXR_0091_0.png', 'CHNCXR_0423_1.png', 'CHNCXR_0466_1.png', 'CHNCXR_0173_0.png', 'CHNCXR_0136_0.png', 'CHNCXR_0272_0.png', 'CHNCXR_0237_0.png', 'CHNCXR_0093_0.png', 'CHNCXR_0586_1.png', 'CHNCXR_0546_1.png', 'CHNCXR_0503_1.png', 'CHNCXR_0016_0.png', 'CHNCXR_0053_0.png', 'CHNCXR_0647_1.png', 'CHNCXR_0602_1.png', 'CHNCXR_0317_0.png', 'CHNCXR_0333_1.png', 'CHNCXR_0376_1.png', 'CHNCXR_0626_1.png', 'CHNCXR_0032_0.png', 'CHNCXR_0077_0.png', 'CHNCXR_0562_1.png', 'CHNCXR_0527_1.png', 'CHNCXR_0352_1.png', 'CHNCXR_0296_0.png', 'CHNCXR_0482_1.png', 'CHNCXR_0197_0.png', 'CHNCXR_0256_0.png', 'CHNCXR_0213_0.png', 'CHNCXR_0392_1.png', 'CHNCXR_0157_0.png', 'CHNCXR_0112_0.png', 'CHNCXR_0407_1.png', 'CHNCXR_0442_1.png', 'CHNCXR_0486_1.png', 'CHNCXR_0193_0.png', 'CHNCXR_0356_1.png', 'CHNCXR_0292_0.png', 'CHNCXR_0036_0.png', 'CHNCXR_0073_0.png', 'CHNCXR_0566_1.png', 'CHNCXR_0523_1.png', 'CHNCXR_0622_1.png', 'CHNCXR_0153_0.png', 'CHNCXR_0116_0.png', 'CHNCXR_0403_1.png', 'CHNCXR_0446_1.png', 'CHNCXR_0252_0.png', 'CHNCXR_0217_0.png', 'CHNCXR_0396_1.png', 'CHNCXR_0097_0.png', 'CHNCXR_0582_1.png', 'CHNCXR_0276_0.png', 'CHNCXR_0233_0.png', 'CHNCXR_0427_1.png', 'CHNCXR_0462_1.png', 'CHNCXR_0177_0.png', 'CHNCXR_0132_0.png', 'CHNCXR_0337_1.png', 'CHNCXR_0372_1.png', 'CHNCXR_0643_1.png', 'CHNCXR_0606_1.png', 'CHNCXR_0313_0.png', 'CHNCXR_0542_1.png', 'CHNCXR_0507_1.png', 'CHNCXR_0012_0.png', 'CHNCXR_0057_0.png', 'CHNCXR_0249_0.png', 'CHNCXR_0335_1.png', 'CHNCXR_0148_0.png', 'CHNCXR_0370_1.png', 'CHNCXR_0418_1.png', 'CHNCXR_0010_0.png', 'CHNCXR_0055_0.png', 'CHNCXR_0540_1.png', 'CHNCXR_0505_1.png', 'CHNCXR_0311_0.png', 'CHNCXR_0641_1.png', 'CHNCXR_0604_1.png', 'CHNCXR_0580_1.png', 'CHNCXR_0639_1.png', 'CHNCXR_0095_0.png', 'CHNCXR_0068_0.png', 'CHNCXR_0538_1.png', 'CHNCXR_0175_0.png', 'CHNCXR_0130_0.png', 'CHNCXR_0425_1.png', 'CHNCXR_0460_1.png', 'CHNCXR_0289_0.png', 'CHNCXR_0274_0.png', 'CHNCXR_0231_0.png', 'CHNCXR_0188_0.png', 'CHNCXR_0559_1.png', 'CHNCXR_0009_0.png', 'CHNCXR_0658_1.png', 'CHNCXR_0308_0.png', 'CHNCXR_0394_1.png', 'CHNCXR_0250_0.png', 'CHNCXR_0215_0.png', 'CHNCXR_0401_1.png', 'CHNCXR_0444_1.png', 'CHNCXR_0369_1.png', 'CHNCXR_0151_0.png', 'CHNCXR_0114_0.png', 'CHNCXR_0290_0.png', 'CHNCXR_0479_1.png', 'CHNCXR_0129_0.png', 'CHNCXR_0354_1.png', 'CHNCXR_0191_0.png', 'CHNCXR_0228_0.png', 'CHNCXR_0484_1.png', 'CHNCXR_0620_1.png', 'CHNCXR_0599_1.png', 'CHNCXR_0564_1.png', 'CHNCXR_0521_1.png', 'CHNCXR_0034_0.png', 'CHNCXR_0071_0.png', 'CHNCXR_0377_1.png', 'CHNCXR_0332_1.png', 'CHNCXR_0502_1.png', 'CHNCXR_0547_1.png', 'CHNCXR_0052_0.png', 'CHNCXR_0017_0.png', 'CHNCXR_0603_1.png', 'CHNCXR_0646_1.png', 'CHNCXR_0316_0.png', 'CHNCXR_0092_0.png', 'CHNCXR_0587_1.png', 'CHNCXR_0467_1.png', 'CHNCXR_0422_1.png', 'CHNCXR_0137_0.png', 'CHNCXR_0172_0.png', 'CHNCXR_0236_0.png', 'CHNCXR_0273_0.png', 'CHNCXR_0212_0.png', 'CHNCXR_0257_0.png', 'CHNCXR_0393_1.png', 'CHNCXR_0113_0.png', 'CHNCXR_0156_0.png', 'CHNCXR_0443_1.png', 'CHNCXR_0406_1.png', 'CHNCXR_0353_1.png', 'CHNCXR_0297_0.png', 'CHNCXR_0483_1.png', 'CHNCXR_0196_0.png', 'CHNCXR_0627_1.png', 'CHNCXR_0662_1.png', 'CHNCXR_0076_0.png', 'CHNCXR_0033_0.png', 'CHNCXR_0526_1.png', 'CHNCXR_0563_1.png', 'CHNCXR_0194_0.png', 'CHNCXR_0481_1.png', 'CHNCXR_0268_0.png', 'CHNCXR_0439_1.png', 'CHNCXR_0295_0.png', 'CHNCXR_0169_0.png', 'CHNCXR_0351_1.png', 'CHNCXR_0524_1.png', 'CHNCXR_0561_1.png', 'CHNCXR_0074_0.png', 'CHNCXR_0031_0.png', 'CHNCXR_0625_1.png', 'CHNCXR_0089_0.png', 'CHNCXR_0660_1.png', 'CHNCXR_0618_1.png', 'CHNCXR_0519_1.png', 'CHNCXR_0049_0.png', 'CHNCXR_0441_1.png', 'CHNCXR_0404_1.png', 'CHNCXR_0329_1.png', 'CHNCXR_0111_0.png', 'CHNCXR_0154_0.png', 'CHNCXR_0391_1.png', 'CHNCXR_0210_0.png', 'CHNCXR_0255_0.png', 'CHNCXR_0028_0.png', 'CHNCXR_0578_1.png', 'CHNCXR_0585_1.png', 'CHNCXR_0090_0.png', 'CHNCXR_0234_0.png', 'CHNCXR_0271_0.png', 'CHNCXR_0498_1.png', 'CHNCXR_0135_0.png', 'CHNCXR_0348_1.png', 'CHNCXR_0170_0.png', 'CHNCXR_0465_1.png', 'CHNCXR_0420_1.png', 'CHNCXR_0375_1.png', 'CHNCXR_0108_0.png', 'CHNCXR_0330_1.png', 'CHNCXR_0458_1.png', 'CHNCXR_0209_0.png', 'CHNCXR_0388_1.png', 'CHNCXR_0314_0.png', 'CHNCXR_0601_1.png', 'CHNCXR_0644_1.png', 'CHNCXR_0050_0.png', 'CHNCXR_0015_0.png', 'CHNCXR_0500_1.png', 'CHNCXR_0545_1.png', 'CHNCXR_0131_0.png', 'CHNCXR_0174_0.png', 'CHNCXR_0288_0.png', 'CHNCXR_0461_1.png', 'CHNCXR_0424_1.png', 'CHNCXR_0230_0.png', 'CHNCXR_0275_0.png', 'CHNCXR_0189_0.png', 'CHNCXR_0581_1.png', 'CHNCXR_0094_0.png', 'CHNCXR_0638_1.png', 'CHNCXR_0069_0.png', 'CHNCXR_0539_1.png', 'CHNCXR_0054_0.png', 'CHNCXR_0011_0.png', 'CHNCXR_0504_1.png', 'CHNCXR_0541_1.png', 'CHNCXR_0310_0.png', 'CHNCXR_0605_1.png', 'CHNCXR_0640_1.png', 'CHNCXR_0248_0.png', 'CHNCXR_0149_0.png', 'CHNCXR_0371_1.png', 'CHNCXR_0334_1.png', 'CHNCXR_0419_1.png', 'CHNCXR_0621_1.png', 'CHNCXR_0598_1.png', 'CHNCXR_0520_1.png', 'CHNCXR_0565_1.png', 'CHNCXR_0070_0.png', 'CHNCXR_0035_0.png', 'CHNCXR_0478_1.png', 'CHNCXR_0291_0.png', 'CHNCXR_0355_1.png', 'CHNCXR_0128_0.png', 'CHNCXR_0190_0.png', 'CHNCXR_0485_1.png', 'CHNCXR_0229_0.png', 'CHNCXR_0395_1.png', 'CHNCXR_0214_0.png', 'CHNCXR_0251_0.png', 'CHNCXR_0445_1.png', 'CHNCXR_0400_1.png', 'CHNCXR_0115_0.png', 'CHNCXR_0368_1.png', 'CHNCXR_0150_0.png', 'CHNCXR_0558_1.png', 'CHNCXR_0008_0.png', 'CHNCXR_0659_1.png', 'CHNCXR_0309_0.png', 'CHNCXR_0117_0.png', 'CHNCXR_0152_0.png', 'CHNCXR_0447_1.png', 'CHNCXR_0402_1.png', 'CHNCXR_0216_0.png', 'CHNCXR_0253_0.png', 'CHNCXR_0397_1.png', 'CHNCXR_0072_0.png', 'CHNCXR_0037_0.png', 'CHNCXR_0522_1.png', 'CHNCXR_0567_1.png', 'CHNCXR_0623_1.png', 'CHNCXR_0487_1.png', 'CHNCXR_0192_0.png', 'CHNCXR_0357_1.png', 'CHNCXR_0293_0.png', 'CHNCXR_0607_1.png', 'CHNCXR_0642_1.png', 'CHNCXR_0312_0.png', 'CHNCXR_0506_1.png', 'CHNCXR_0543_1.png', 'CHNCXR_0056_0.png', 'CHNCXR_0013_0.png', 'CHNCXR_0373_1.png', 'CHNCXR_0336_1.png', 'CHNCXR_0232_0.png', 'CHNCXR_0277_0.png', 'CHNCXR_0463_1.png', 'CHNCXR_0426_1.png', 'CHNCXR_0133_0.png', 'CHNCXR_0176_0.png', 'CHNCXR_0096_0.png', 'CHNCXR_0583_1.png']\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "data_directory = random = os.getenv(\"DATASET1\")\n",
    "all_files = np.array(os.listdir(data_directory))\n",
    "\n",
    "\n",
    "if os.path.exists(data_directory):\n",
    "    print(\"Directory exists:\", data_directory)\n",
    "    print(os.listdir(data_directory))\n",
    "else:\n",
    "    print(\"Directory does not exist:\", data_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how we want to prepare our images for the neural network\n",
    "# This converts the image to a format the network can understand\n",
    "data_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "\n",
    "# This function loads and prepares each image for our neural network\n",
    "def CNNloader(data_root, filename):\n",
    "    # Load the image and resize it to 224x224 pixels\n",
    "    filename_actual = data_root + '/' + filename\n",
    "    data_old = io.imread(filename_actual)\n",
    "    data_old = resize(data_old, (224, 224))\n",
    "    data_old = np.array(data_old, dtype=np.float32)\n",
    "    \n",
    "    # If the image is black and white, convert it to a color image\n",
    "    # (Our neural network expects color images)\n",
    "    if len(data_old.shape) <= 2:\n",
    "        data_new = np.zeros(data_old.shape + (3,))\n",
    "        data_new[:,:,0] = data_new[:,:,1] = data_new[:,:,2] = np.array(data_old)\n",
    "        data_old = np.array(data_new, dtype=np.float32)\n",
    "    \n",
    "    # Apply our predefined transformations\n",
    "    data_old = data_transforms(np.array(data_old))\n",
    "    return data_old\n",
    "\n",
    "# This class helps organize our data and labels\n",
    "class CNNDataLayer(data.Dataset):\n",
    "    def __init__(self, data_root, filenames, loader):\n",
    "        self.data_root = data_root\n",
    "        self.filenames = filenames\n",
    "        self.loader = loader\n",
    "\n",
    "    # This tells the computer how to get each image and its label\n",
    "    def __getitem__(self, index):\n",
    "        filename = self.filenames[index]\n",
    "        # We assume the label (0 or 1) is the second-to-last character in the filename\n",
    "        target = [int(filename[-5])]\n",
    "        target = torch.from_numpy(np.array(target))\n",
    "        data = self.loader(self.data_root, filename)\n",
    "        return data, target\n",
    "\n",
    "    # This tells the computer how many images we have\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7,) (7,)\n",
      "Epoch is 1\n",
      "Epoch is 2\n",
      "Epoch is 3\n",
      "Epoch is 4\n",
      "Epoch is 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = None\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')  # Use Metal Performance Shaders on macOS\n",
    "else:\n",
    "    device = torch.device('cpu')  # Fallback to CPU\n",
    "\n",
    "# print(\"Using device:\", device)\n",
    "# Get a list of all our image files and shuffle them randomly\n",
    "all_files = np.array(os.listdir(data_directory))\n",
    "np.random.shuffle(all_files)\n",
    "\n",
    "# Split our data: 70% for training, 30% for testing\n",
    "# train_files = all_files[:round(0.7*all_files.shape[0])]\n",
    "train_files = all_files[:round(0.01*all_files.shape[0])]\n",
    "test_files = all_files[round(0.99*all_files.shape[0]):]\n",
    "print(train_files.shape, test_files.shape)\n",
    "\n",
    "# Create our datasets using the custom class we defined earlier\n",
    "data_sets_train = CNNDataLayer(data_root=data_directory, filenames=train_files, loader=CNNloader)\n",
    "data_sets_test = CNNDataLayer(data_root=data_directory, filenames=test_files, loader=CNNloader)\n",
    "\n",
    "# Create data loaders that will help feed data to our neural network in batches\n",
    "data_loaders_train = data.DataLoader(data_sets_train, batch_size=8, shuffle=True, num_workers=0)\n",
    "data_loaders_test = data.DataLoader(data_sets_test, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "# Load a pre-trained neural network (VGG16) that's good at understanding images\n",
    "model_to_train = models.vgg16(pretrained=True)\n",
    "# model_to_train.load_state_dict(torch.load(\"./vgg16-397923af.pth\"))\n",
    "\n",
    "# We don't want to retrain the whole network, so we \"freeze\" most of it\n",
    "for param in model_to_train.features.parameters():\n",
    "    param.require_grad = False\n",
    "\n",
    "# Modify the last part of the network to focus on our specific task (finding tuberculosis)\n",
    "num_features = model_to_train.classifier[6].in_features\n",
    "features = list(model_to_train.classifier.children())[:-1]\n",
    "features.extend([nn.Linear(num_features, 1), nn.Sigmoid()])\n",
    "model_to_train.classifier = nn.Sequential(*features)\n",
    "\n",
    "# Alternatively, we could use a custom-made neural network\n",
    "# from own_cnn_model import SimpleCNN\n",
    "# model_to_train = SimpleCNN()\n",
    "model_to_train = model_to_train.to(device)\n",
    "\n",
    "# Set up the \"loss function\" (how the network measures its mistakes)\n",
    "# and the \"optimizer\" (how the network learns from its mistakes)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_ft = optim.SGD(model_to_train.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Start the training process\n",
    "num_epochs = 5  # We'll train for 100 rounds\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print('Epoch is ' + str(epoch))\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    total_misclassified_train = total_count_train = 0\n",
    "    total_misclassified_test = total_count_test = 0\n",
    "    \n",
    "    # Training phase\n",
    "    model_to_train.train()\n",
    "    for batch_idx, (data_now, target_now) in enumerate(data_loaders_train):\n",
    "        # Move data to GPU if available\n",
    "        data_now, target_now = data_now.to(device), target_now.to(device)\n",
    "        \n",
    "        # Make predictions\n",
    "        target_output_model = model_to_train(data_now)\n",
    "        target_now = target_now.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        # Calculate loss (how wrong the predictions were)\n",
    "        target_loss = criterion(target_output_model, target_now)\n",
    "        \n",
    "        # Calculate how many predictions were wrong\n",
    "        misclassified_temp = target_output_model[target_now == 1]\n",
    "        total_misclassified_train += (misclassified_temp < 0.5).sum().item()\n",
    "        misclassified_temp = target_output_model[target_now == 0]\n",
    "        total_misclassified_train += (misclassified_temp >= 0.5).sum().item()\n",
    "        total_count_train += data_now.shape[0]\n",
    "        \n",
    "        # Learn from the mistakes (backpropagation)\n",
    "        optimizer_ft.zero_grad()\n",
    "        target_loss.backward()\n",
    "        optimizer_ft.step()\n",
    "    \n",
    "    # Testing phase (similar to training, but we don't learn from these)\n",
    "    model_to_train.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data_now, target_now) in enumerate(data_loaders_test):\n",
    "            data_now, target_now = data_now.to(device), target_now.to(device)\n",
    "            target_output_model = model_to_train(data_now)\n",
    "            \n",
    "            # Calculate how many predictions were wrong\n",
    "            misclassified_temp = target_output_model[target_now == 1]\n",
    "            total_misclassified_test += (misclassified_temp < 0.5).sum().item()\n",
    "            misclassified_temp = target_output_model[target_now == 0]\n",
    "            total_misclassified_test += (misclassified_temp >= 0.5).sum().item()\n",
    "            total_count_test += data_now.shape[0]\n",
    "    \n",
    "    # Save our progress (the network's current state)\n",
    "    snapshot_path = './snapshots_trial'\n",
    "    os.makedirs(snapshot_path, exist_ok=True)\n",
    "    snapshot_name = f'epoch-{epoch}-trainerror-{total_misclassified_train/total_count_train:.4f}-testerror-{total_misclassified_test/total_count_test:.4f}.pth'\n",
    "    torch.save(model_to_train.state_dict(), os.path.join(snapshot_path, snapshot_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bt/f868ty6d66g2scxrj62lpgzc0000gn/T/ipykernel_88117/3568264980.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_loaded = torch.load(\"./snapshots/epoch-5-trainerror-0.1447-testerror-0.1558.pth\", map_location=lambda storage, loc: storage)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './snapshots/epoch-5-trainerror-0.1447-testerror-0.1558.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example of how to load a saved model for later use\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model_loaded \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./snapshots/epoch-5-trainerror-0.1447-testerror-0.1558.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model_to_train\u001b[38;5;241m.\u001b[39mload_state_dict(model_loaded)\n",
      "File \u001b[0;32m~/MACHINE_LEARNING_ENVIRONMENT/env/lib/python3.12/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/MACHINE_LEARNING_ENVIRONMENT/env/lib/python3.12/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/MACHINE_LEARNING_ENVIRONMENT/env/lib/python3.12/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './snapshots/epoch-5-trainerror-0.1447-testerror-0.1558.pth'"
     ]
    }
   ],
   "source": [
    "# Example of how to load a saved model for later use\n",
    "model_loaded = torch.load(\"./snapshots/epoch-5-trainerror-0.2857-testerror-0.4286.pth\", map_location=lambda storage, loc: storage)\n",
    "model_to_train.load_state_dict(model_loaded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
